\documentclass{beamer}


\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{lmodern}
\usepackage{amsmath, amssymb}
\usepackage{booktabs}
\usepackage{array}


\usetheme{Madrid}
\usecolortheme{default}


\title[Deletion Evaluation with GLMs]{Evaluating Deletions with a DNA Masked Language Model}
\institute{Project 6}
\date{\today}

% -------------------------------------------------
\begin{document}

% -------------------------------------------------
\begin{frame}
 \titlepage
\end{frame}

% -------------------------------------------------
\section{Evaluation of Deletions}

%------------------------------------------------
\begin{frame}{Goal of the Evaluation}
\textbf{Goal:}  
Quantify how \textbf{deletions} affect a DNA masked language model (GLM).

\vspace{0.3cm}

We compare two sequences:
\begin{itemize}
 \item \textbf{Reference (ref):} undeleted sequence
 \item \textbf{Perturbed (alt):} same sequence with deletions (encoded as `-`)
\end{itemize}

Both sequences are processed identically by the model.

\vspace{0.3cm}

\textbf{Key idea:}  
Any difference in model behavior can be attributed directly to the deletion.
\end{frame}

%------------------------------------------------
\begin{frame}{Method 1: Fast $\Delta$ Log-Likelihood Score}
\textbf{Question:}  
Does a deletion make the sequence globally less plausible for the model?

\vspace{0.3cm}

We compute a fast, global score by comparing the summed log-probabilities of
the reference and perturbed sequences.

\vspace{0.3cm}

\textbf{Definition:}
\[
\Delta = \sum_{i=1}^{L} \log p_{\text{model}}(x_i^{\text{alt}})
       - \sum_{i=1}^{L} \log p_{\text{model}}(x_i^{\text{ref}})
\]

\vspace{0.2cm}

\textbf{Interpretation:}
\begin{itemize}
 \item $\Delta \ll 0$: deletion strongly disrupts the sequence
 \item $\Delta \approx 0$: deletion has little global effect
\end{itemize}
\end{frame}

%------------------------------------------------
\begin{frame}{Why the $\Delta$ Log-Likelihood Score is Useful}
\begin{itemize}
 \item Requires only one forward pass per sequence
 \item Very fast and scalable
 \item Suitable for screening and ranking deletions
\end{itemize}

\vspace{0.3cm}

\textbf{Note:}  
Because the model is a masked language model, this is a \emph{pseudo-likelihood},
not a true generative likelihood.

\vspace{0.2cm}

Nevertheless, it is effective as a \textbf{comparative score} between ref and alt.
\end{frame}

%------------------------------------------------
\begin{frame}{Method 2: Influence / Probability-Shift Score}
\textbf{Question:}  
Where and how does a deletion change the modelâ€™s predictions?

\vspace{0.3cm}

Instead of scoring the whole sequence at once, we analyze
\textbf{position-specific changes in predicted nucleotide distributions}.

\vspace{0.3cm}

For each target position $j$:
\begin{enumerate}
 \item Mask position $j$ in the reference sequence
 \item Mask position $j$ in the perturbed sequence
 \item Compare the predicted distributions
\end{enumerate}

\vspace{0.2cm}

This captures both \textbf{local and non-local effects} of deletions.
\end{frame}

%------------------------------------------------
\begin{frame}{Influence Score: Mathematical Form}
Let $p_{\text{ref}}(v)$ and $p_{\text{alt}}(v)$ be the predicted probabilities
for nucleotide $v \in \{A,C,G,T,-\}$ at position $j$.

\vspace{0.3cm}

\textbf{Default shift metric:}
\[
\text{shift}(j) = \max_v \left| \log p_{\text{alt}}(v) - \log p_{\text{ref}}(v) \right|
\]

\vspace{0.3cm}

\textbf{Final influence score:}
\[
\text{Influence} = \frac{1}{N} \sum_{j \in \text{targets}} \text{shift}(j)
\]

\vspace{0.2cm}

Higher scores indicate stronger or more widespread effects of the deletion.
\end{frame}

%------------------------------------------------
\begin{frame}{Why We Use Both Scores}
The two scores answer complementary questions:

\vspace{0.3cm}

\begin{center}
\begin{tabular}{l l l}
\toprule
\textbf{Score} & \textbf{Question} & \textbf{Strength} \\
\midrule
$\Delta$ log-likelihood & Global disruption? & Fast, robust \\
Influence score & Where/how predictions change? & Local, mechanistic \\
\bottomrule
\end{tabular}
\end{center}

\vspace{0.4cm}

Together, they provide both a global and a local view of deletion effects.
\end{frame}

%------------------------------------------------
\begin{frame}{Model Quality Score (Step 2)}
Before interpreting deletion effects, we evaluate the model itself.

\vspace{0.3cm}

Using held-out sequences with random masking, we compute:
\begin{itemize}
 \item Masked language modeling (MLM) loss
 \item Perplexity
 \item Masked-token accuracy
\end{itemize}

\vspace{0.3cm}

\textbf{Purpose:}
\begin{itemize}
 \item Sanity check that the model learned meaningful sequence structure
 \item Enable comparison between different models or training setups
\end{itemize}
\end{frame}

%------------------------------------------------
\begin{frame}{Next Steps}
\begin{itemize}
 \item Scale evaluation to larger datasets and more deletions
 \item Compare scores across different models or architectures
 \item Analyze positional patterns of high influence scores
 \item Relate model-based scores to biological annotations
\end{itemize}

\vspace{0.3cm}
\end{frame}

% -------------------------------------------------
\end{document}
