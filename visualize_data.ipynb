{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1221a6b080fe1a3b",
   "metadata": {},
   "source": [
    "# Training and Running of new GLM model\n",
    "This Notebook is for training and running the our new GLM model that includes (for now only) deletion tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa85af1147b2b48f",
   "metadata": {},
   "source": [
    "## Adding necessary imports\n",
    "You can run this block to import necessary classes and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93442bee417af6d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fundemental_classes.glm_model import GLMModel\n",
    "\n",
    "model = GLMModel(\"./dna_bert_final\", \"simulated_sequences/augumented_sequence_size10000_length150_deletions0.1_nodeletionseq0.25.fasta\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbcd7620dbcc72f",
   "metadata": {},
   "source": [
    "## Training\n",
    "We use `Bert` model to create the embeddings and train it using masking to get nicely trained model. If you want to change the size of training data, please look into `simulated_sequences` directory. \\\\\\\n",
    "*important:* Training is not necessary if you already have a trained model. You can load it using the `GLMModel`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aa337134c4e64b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train(epochs=30, batch_size=16, lr=2e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c782982ae3842392",
   "metadata": {},
   "source": [
    "## Visualization\n",
    "This block is used for getting already trained model, passing the sequence to test how well the model performs.\n",
    "\\\\\\\\\\\n",
    "*Important:* For now we pass our test cases by hand. However in the future we will generate good inputs to test how well the model functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from fundemental_classes.sequence_plotter import SequenceLogoPlotter\n",
    "\n",
    "plotter = SequenceLogoPlotter()\n",
    "\n",
    "header = \">seq0005|label=both|posAmotif=41|posBmotif=98|gaplength=50\"\n",
    "sequence = \"ATCAACTGTTAGGTAACCATTTGCCCGCCAACTACAAGTACATATTCACATCACGAATCGGGCGGAAAACCTGAGACCGACTGATGC--ATGTGGTAGGTACTGCGCGACGTGCCGGC\"\n",
    "\n",
    "prob_matrix = model.get_full_reconstruction_probs(sequence)\n",
    "\n",
    "plotter.plot(header, sequence, prob_matrix, motif_length=7)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "p06_indel_GLM_W2526",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
